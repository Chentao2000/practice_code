## Hadoop的文件 IO:

1. Hadoop 是如何保证数据的完整的？

检测数据完整性的常见方法：在数据第一次引入系统时计算校验和(checksum)，并在每一次通道传输时再次计算，比较两次校验和是否匹配，这只能检测数据是否有损坏，而不能修复数据

常见的错误检测码是CRC-32（32位循环冗余校验）,任何大小的数据都计算获取32位大小的校验和

Hadoop ChecksumFileSystem使用CRC-32, Hadoop HDFS使用更有效的变体CRC-32C


2. Hadoop 对序列化机制的要求有那些？

Hadoop的序列化机制与java的序列化机制不同，它将对象序列化到流中，值得一提的是java的序列化机制是不断的创建对象，但在Hadoop的序列化机制中，用户可以复用对象，这样就减少了java对象的分配和回收，提高了应用效率。

对于需要保存和处理大规模数据的Hadoop来说，其序列化机制要达到以下目的：

-     排列紧凑：尽量减少带宽，加快数据交换速度
-     处理快速：进程间通信需要大量的数据交互，使用大量的序列化机制，必须减少序列化和反序列的开支
-     跨语言：可以支持不同语言间的数据交互啊，如C++
-     可扩展：当系统协议升级，类定义发生变化，序列化机制需要支持这些升级和变化


3. 如何自定义Hadoop数据类型？


- 继承接口Writable,实现其方法write()和readFields(), 以便该数据能被序列化后完成网络传输或文件输入/输出；

- 如果该数据需要作为主键key使用，或需要比较数值大小时，则需要实现WritalbeComparable接口,实现其方法write(),readFields(),CompareTo() 。

 - 数据类型，必须要有一个无参的构造方法，为了方便反射，进行创建对象。    

 - 在自定义数据类型中，建议使用java的原生数据类型，最好不要使用Hadoop对原生类型进行封装的数据类型。比如 int x ;//IntWritable 和String s; //Text  等等
